{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90c1ca68-770e-4065-9e56-6066a1114804",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import PyPDF2\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1dd6e19-ca9e-4073-b409-859b036f6204",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf_to_dataframe(pdf_path, custom_exclusion):\n",
    "\n",
    "    # Open the PDF file\n",
    "    pdf_document = fitz.open(pdf_path)\n",
    "\n",
    "    # Initialize NLTK sentence tokenizer\n",
    "    nltk.download('punkt')  # Download the Punkt tokenizer models\n",
    "    sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "    # Extract text from each page\n",
    "    text_content = \"\"\n",
    "    for page_num in range(pdf_document.page_count):\n",
    "        page = pdf_document[page_num]\n",
    "        text_content += page.get_text()\n",
    "\n",
    "    # Tokenize the text into sentences\n",
    "    sentences = sentence_tokenizer.tokenize(text_content)\n",
    "\n",
    "    # Apply custom exclusion\n",
    "    for phrase in custom_exclusion:\n",
    "        sentences = [re.sub(re.escape(phrase), '', sentence, flags=re.IGNORECASE) for sentence in sentences]\n",
    "\n",
    "    # Create a DataFrame with appropriate column names\n",
    "    text_df = pd.DataFrame(sentences, columns=['Sentence'])\n",
    "\n",
    "    return text_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71a64f7-f327-4473-aba1-57e70c461b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pdf_content(pdf_folder):\n",
    "    # Create an empty DataFrame to store the data\n",
    "    pdf_data = pd.DataFrame(columns=['pdf_name', 'pdf_content'])\n",
    "\n",
    "    # Loop through the PDF files in the folder\n",
    "    for pdf_file in os.listdir(pdf_folder):\n",
    "        if pdf_file.endswith('.pdf'):\n",
    "            pdf_path = os.path.join(pdf_folder, pdf_file)\n",
    "            pdf_content = extract_text_from_pdf(pdf_path)\n",
    "            pdf_data = pdf_data.append({'pdf_name': pdf_file, 'pdf_content': pdf_content}, ignore_index=True)\n",
    "\n",
    "    return pdf_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027bb033-8976-474a-8fba-87b4e4d07620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the folder containing PDF files\n",
    "pdf_folder = 'dataset'\n",
    "\n",
    "# Call the function to extract PDF content and get the DataFrame\n",
    "pdf_data = extract_pdf_content(pdf_folder)\n",
    "\n",
    "pdf_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9dbdad94-9018-4070-bda2-f6d07c880940",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_filename = 'Data_mesh_csv.csv'\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "pdf_data.to_csv(csv_filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac01a9f-1db0-4113-a4b8-c133a3dedfcc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
